# <Сумма значений по строкам матрицы>

- Student: Лузан Егор Андреевич, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 11

## 1. Introduction
Цель работы — реализовать и сравнить две версии программы, вычисляющей сумму элементов в каждой строке матрицы:
1. последовательную,
2. параллельную, использующую библиотеку **MPI**.

Подобная задача встречается при обработке больших таблиц данных, изображений, численных расчётах.

Ожидается ускорение mpi версии относительно последовательной версии.

## 2. Problem Statement
Дана матрица A размера `height × width`.
Требуется вычислить вектор $sum$ размера `width`, где:
$$sum[i] = \sum_{j=0}^{width-1} A_{i\_j}$$ 

**Входные данные:**
- Матрица в форме вектора `(std::vector)` размера `height × width` состоящая из целых чисел.

**Выходные данные**:
-  Вектор `(std::vector)` размера `height` состоящий из целых чисел, каждое из которых является суммой строки матрицы.

**Ограничения:**
- элементы матрицы — целые числа.
- элементы матрицы лежат в диапазоне -`[0; 42000]`

## 3. Baseline Algorithm (Sequential)
- Инициализация всех элементов выходного вектора нулями.
- Последовательный обход всех строк исходной матрицы, производя суммирование элементов в них и записывая результат в соответствующий элемент выходного вектора.

## 4. Parallelization Scheme
**Распределение данных:**
- Для каждого процесса:
	1. Делим высоту матрицы нацело на количество проецессов, получая `rows_per_proc`.
	2. Получаем остаток `rest` от предыдущего деления.
	3. Вычисляем суммы строк из блока, за который отвечает текущий процесс (`begin` - первая строка блока, `end` - последняя строка блока). Если остаток не равен 0, то все процессы с `rank` <  `rest` получат `rows_per_proc + 1` строк матрицы. Остальные получат `rows_per_proc` строк.

**Роли рангов:**
- Во время вычисления сумм процессы всех рангов имеют одинаковые задачи.
- После вычисления итоговый результат будет записан в процессе `rank = 0` с помощью `MPI_Reduce()`.

## 5. Implementation Details
- Code structure (files, key classes/functions)
- Important assumptions and corner cases
- Memory usage considerations

## 6. Experimental Setup
- Hardware/OS: 
	- CPU: Intel Core i7-13620H; P-cores-6, E-cores-4,
	- RAM: 16 GB RAM,
	- OS: Windows 11, x64.
- Toolchain:
	- Cmake 3.28.3,
	- Компилятор: gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0,
	- Использовался Docker-контейнер с Ubuntu 24.04.2 LTS,
	- Режим сборки: Release.

- Data: Для замера производительности использовалась матрица размером 10 000 × 10 000, элементы которой представляют собой целые последовательные числа от 0 до 42 000.

## 7. Results and Discussion

### 7.1 Correctness
Корректность работы проверена с помощью GoogleTest на матрицах размерностями 3×3, 2×5, 10×70, 2000×5, 5×2000, 1×1. 

### 7.2 Performance

| Mode | Count | Time, s            | Speedup | Efficiency |
| ---- | ----- | ------------------ | ------- | ---------- |
| seq  | 1     | 0.05721            | 1.00    | N/A        |
| mpi  | 2     | 0.030963           | 1.84    | 92%        |
| mpi  | 4     | 0.02039            | 2.8     | 70%        |

## 8. Conclusions

Разработана программа, выполняющая поиск сумм строк матрицы. Также разработана её параллельная версия с использованием MPI. Эффективность параллельного алгоритма высока, однако на практике не достигает 100% из-за накладных расходов на создание процессов.

## 9. References
1. Курс лекций ННГУ "Параллельное программирование для кластерных систем".
2. Стандарт MPI.